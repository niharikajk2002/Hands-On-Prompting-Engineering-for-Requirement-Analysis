{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': \"\\nProvide a requirement analysis for building an AI-powered career counseling assistant that uses Self-Consistency. The assistant should generate multiple responses for the same query and assess which answer is the most consistent. It should weigh the responses based on factors such as accuracy, relevance to the user's goals, and alignment with current job market trends. For example:\\n\\nUser asks: 'What’s the best career path in tech for someone with a background in data science?'\\nThe assistant generates multiple responses, including options like data engineering, machine learning engineering, and data analytics roles.\\nThe assistant evaluates which response is most consistent with the user’s skills, goals, and current job market trends, providing the most accurate and relevant recommendation.\\n\", 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 100, 'num_predict': 100}}\n",
      "Here's an updated version of the passage:\n",
      "\n",
      "As a conversational AI, I'm designed to provide personalized guidance and recommendations. When a user shares their interests, skills, and career aspirations, I use this information to offer tailored suggestions.\n",
      "\n",
      "For instance, let's say a user expresses interest in pursuing a career in data science. They mention that they have experience with Python and are interested in machine learning. In response, I might recommend various courses or certifications that can help them enhance their skills in this area\n",
      "Time taken: 30.178s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from _pipeline import create_payload, model_req\n",
    " \n",
    "PROMPT = f\"\"\"\n",
    "Provide a requirement analysis for building an AI-powered career counseling assistant that uses Self-Consistency. The assistant should generate multiple responses for the same query and assess which answer is the most consistent. It should weigh the responses based on factors such as accuracy, relevance to the user's goals, and alignment with current job market trends. For example:\n",
    "\n",
    "User asks: 'What’s the best career path in tech for someone with a background in data science?'\n",
    "The assistant generates multiple responses, including options like data engineering, machine learning engineering, and data analytics roles.\n",
    "The assistant evaluates which response is most consistent with the user’s skills, goals, and current job market trends, providing the most accurate and relevant recommendation.\n",
    "\"\"\"\n",
    "\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
